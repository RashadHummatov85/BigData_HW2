{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2f9741-4896-4ca4-8f44-09162cc5ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/25 22:09:38 WARN Utils: Your hostname, MSI, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/25 22:09:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/25 22:09:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Initialized Successfully!\n",
      "Spark Version: 4.0.1\n",
      "\n",
      "⚠️ Data loading failed. Check your file path.\n",
      "[PATH_NOT_FOUND] Path does not exist: file:/mnt/c/Users/rasha/Downloads/yellow_tripdata_2016-03.csv. SQLSTATE: 42K03\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# --- FORCE JAVA 17 ---\n",
    "# This explicitly tells the notebook where to find the correct Java version\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "# ---------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "print(\"Initializing Spark...\")\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HW2_Task1_RDD\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel(\"ERROR\") \n",
    "    \n",
    "    print(\"✅ Spark Initialized Successfully!\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Error still occurring. Please read below:\")\n",
    "    print(e)\n",
    "\n",
    "# 2. Load the Dataset\n",
    "# (Make sure 'yellow_tripdata_2016-03.csv' is in the folder where you started Jupyter)\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"yellow_tripdata_2016-03.csv\")\n",
    "\n",
    "    raw_rdd = df.rdd\n",
    "    print(f\"✅ Data Loaded. Total Rows: {raw_rdd.count()}\")\n",
    "    print(f\"First Row: {raw_rdd.first()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n⚠️ Data loading failed. Check your file path.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf15b9e-5fb8-49cc-be04-f5b922d9cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: archive (13)/yellow_tripdata_2016-03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Data Loaded. Total Rows: 12210952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Row: Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2016, 3, 1, 0, 0), tpep_dropoff_datetime=datetime.datetime(2016, 3, 1, 0, 7, 55), passenger_count=1, trip_distance=2.5, pickup_longitude=-73.97674560546875, pickup_latitude=40.76515197753906, RatecodeID=1, store_and_fwd_flag='N', dropoff_longitude=-74.00426483154297, dropoff_latitude=40.74612808227539, payment_type=1, fare_amount=9.0, extra=0.5, mta_tax=0.5, tip_amount=2.05, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=12.35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Updated path to include the folder you mentioned earlier\n",
    "file_path = \"archive (13)/yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "print(f\"Attempting to load: {file_path}\")\n",
    "\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(file_path)\n",
    "\n",
    "    raw_rdd = df.rdd\n",
    "    print(f\"✅ Success! Data Loaded. Total Rows: {raw_rdd.count()}\")\n",
    "    print(f\"First Row: {raw_rdd.first()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Still not found. Let's find the file location below.\")\n",
    "    import os\n",
    "    print(\"Files in current folder:\")\n",
    "    print(os.listdir(\".\"))\n",
    "    if os.path.exists(\"archive (13)\"):\n",
    "        print(\"\\nFiles in 'archive (13)':\")\n",
    "        print(os.listdir(\"archive (13)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131a2aa8-ae1e-4b92-aeb0-172301dd8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA ===\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "\n",
      "=== SAMPLE DATA ===\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude  |pickup_latitude   |RatecodeID|store_and_fwd_flag|dropoff_longitude |dropoff_latitude |payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|1       |2016-03-01 00:00:00 |2016-03-01 00:07:55  |1              |2.5          |-73.97674560546875|40.76515197753906 |1         |N                 |-74.00426483154297|40.74612808227539|1           |9.0        |0.5  |0.5    |2.05      |0.0         |0.3                  |12.35       |\n",
      "|1       |2016-03-01 00:00:00 |2016-03-01 00:11:06  |1              |2.9          |-73.98348236083984|40.76792526245117 |1         |N                 |-74.00594329833984|40.7331657409668 |1           |11.0       |0.5  |0.5    |3.05      |0.0         |0.3                  |15.35       |\n",
      "|2       |2016-03-01 00:00:00 |2016-03-01 00:31:06  |2              |19.98        |-73.78202056884766|40.64480972290039 |1         |N                 |-73.97454071044922|40.6757698059082 |1           |54.5       |0.5  |0.5    |8.0       |0.0         |0.3                  |63.8        |\n",
      "|2       |2016-03-01 00:00:00 |2016-03-01 00:00:00  |3              |10.78        |-73.86341857910156|40.769813537597656|1         |N                 |-73.96965026855469|40.75776672363281|1           |31.5       |0.0  |0.5    |3.78      |5.54        |0.3                  |41.62       |\n",
      "|2       |2016-03-01 00:00:00 |2016-03-01 00:00:00  |5              |30.43        |-73.97174072265625|40.79218292236328 |3         |N                 |-74.17716979980469|40.69505310058594|1           |98.0       |0.0  |0.0    |0.0       |15.5        |0.3                  |113.8       |\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== STATISTICS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=====================================================>   (14 + 1) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|   passenger_count|     trip_distance|       fare_amount|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|          12210952|          12210952|          12210952|\n",
      "|   mean| 1.659580022917132| 6.131769762914482|12.795078448428898|\n",
      "| stddev|1.3121892188373814|6156.4826446949155|134.09792335003755|\n",
      "|    min|                 0|               0.0|            -376.0|\n",
      "|    max|                 9|      1.90726288E7|         429496.72|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. View the Schema (Structure)\n",
    "# This shows you the column names and if they are Integers, Doubles (floats), or Strings.\n",
    "print(\"=== SCHEMA ===\")\n",
    "df.printSchema()\n",
    "\n",
    "# 2. View the First 5 Rows\n",
    "# 'truncate=False' ensures you see the full content of every cell.\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# 3. Summary Statistics\n",
    "# This calculates count, mean, min, and max for key columns. \n",
    "# It helps you spot weird data (like negative distances).\n",
    "print(\"\\n=== STATISTICS ===\")\n",
    "df.select(\"passenger_count\", \"trip_distance\", \"fare_amount\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279ee23a-58ec-4b56-bf76-ba164efc4d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Filter complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (We are processing 12139826 valid trips...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Filter() Transformation [cite: 26]\n",
    "# Objective: Remove noisy records.\n",
    "# We keep only trips where trip_distance > 0.\n",
    "# Note: logical operators like 'and' work in Python lambdas, unlike SQL.\n",
    "filtered_rdd = raw_rdd.filter(lambda row: row['trip_distance'] is not None and row['trip_distance'] > 0)\n",
    "\n",
    "print(f\"1. Filter complete.\")\n",
    "print(f\"   (We are processing {filtered_rdd.count()} valid trips...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf91af3-e92d-4c6a-8218-535538c1fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Map complete. Sample: [(1, 2.5), (1, 2.9), (2, 19.98), (2, 10.78), (2, 30.43)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. Map() Transformation [cite: 25]\n",
    "# Objective: Derive a new feature or structured tuple.\n",
    "# We extract a tuple of (VendorID, TripDistance) to analyze who drives further.\n",
    "vendor_distance_pair = filtered_rdd.map(lambda row: (row['VendorID'], float(row['trip_distance'])))\n",
    "\n",
    "print(f\"2. Map complete. Sample: {vendor_distance_pair.take(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4055277-7c4a-4ac7-b844-b55d0a1e44c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. FlatMap complete. Sample elements: ['2016-03-01', '00:00:00', '2016-03-01', '00:00:00', '2016-03-01', '00:00:00', '2016-03-01', '00:00:00', '2016-03-01', '00:00:00']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 3. FlatMap() Transformation [cite: 27]\n",
    "# Objective: Break complex fields into smaller elements.\n",
    "# We turn the single datetime field (e.g., \"2016-03-01 00:00:00\") into a list of [Date, Time].\n",
    "# Since Spark loaded it as a real datetime object, we convert to str() first to split it.\n",
    "def split_date_time(row):\n",
    "    dt_string = str(row['tpep_pickup_datetime']) # Converts to '2016-03-01 00:00:00'\n",
    "    return dt_string.split(' ') # Returns list ['2016-03-01', '00:00:00']\n",
    "\n",
    "flat_mapped_rdd = filtered_rdd.flatMap(split_date_time)\n",
    "\n",
    "print(f\"3. FlatMap complete. Sample elements: {flat_mapped_rdd.take(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c16201d-0be5-4b95-8427-9af3911d0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ReduceByKey() Transformation [cite: 28]\n",
    "# Objective: Compute summary statistics (Aggregations).\n",
    "# We sum the total miles driven for each VendorID.\n",
    "# The input is the (VendorID, Distance) pairs from step 2.\n",
    "total_distance_by_vendor = vendor_distance_pair.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b93ffc7-aeef-4280-8d62-52efc2c76048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS: Total Miles per Vendor ===\n",
      "Vendor 1: 55,384,892.00 miles\n",
      "Vendor 2: 19,489,854.25 miles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 5. Collect() Action [cite: 29]\n",
    "# Objective: Extract outputs to the driver for printing.\n",
    "# Note: meaningful for analysis as it brings the aggregated results back to you.\n",
    "results = total_distance_by_vendor.collect()\n",
    "\n",
    "print(\"\\n=== FINAL RESULTS: Total Miles per Vendor ===\")\n",
    "for vendor, miles in results:\n",
    "    print(f\"Vendor {vendor}: {miles:,.2f} miles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007abc63-3b56-4fe9-b1aa-10016514b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table 'taxi_trips' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- TASK 2: SPARK SQL (30 Points) ---\n",
    "\n",
    "# 1. Create a Temporary View\n",
    "# This allows us to use standard SQL queries on the DataFrame\n",
    "df.createOrReplaceTempView(\"taxi_trips\")\n",
    "print(\"✅ Table 'taxi_trips' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8cb08f-6da9-46de-b743-d066703bd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query 1: Aggregation (Avg Fare by Passenger Count) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-----------+\n",
      "|passenger_count|avg_fare|total_trips|\n",
      "+---------------+--------+-----------+\n",
      "|              0|   25.92|        608|\n",
      "|              1|   12.61|    8690240|\n",
      "|              2|   13.44|    1730864|\n",
      "|              3|   13.43|     499376|\n",
      "|              4|   13.44|     239827|\n",
      "|              5|   12.85|     644712|\n",
      "|              6|   12.64|     405255|\n",
      "|              7|   42.96|         24|\n",
      "|              8|   54.18|         24|\n",
      "|              9|   55.29|         22|\n",
      "+---------------+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 2. Aggregation Query (10 pts)\n",
    "# Requirement: Average value by category, count by class, etc.\n",
    "# We will calculate the Average Fare and Total Trips for each Passenger Count group.\n",
    "print(\"\\n--- Query 1: Aggregation (Avg Fare by Passenger Count) ---\")\n",
    "q1 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        passenger_count,\n",
    "        ROUND(AVG(fare_amount), 2) as avg_fare,\n",
    "        COUNT(*) as total_trips\n",
    "    FROM taxi_trips\n",
    "    WHERE passenger_count IS NOT NULL\n",
    "    GROUP BY passenger_count\n",
    "    ORDER BY passenger_count ASC\n",
    "\"\"\")\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8480d2b-0e62-4a80-a7ad-4c8b24d7cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query 2: Filtering (High Value Trips) ---\n",
      "+--------------------+-------------+-----------+------------+\n",
      "|tpep_pickup_datetime|trip_distance|fare_amount|payment_type|\n",
      "+--------------------+-------------+-----------+------------+\n",
      "| 2016-03-01 00:00:00|        19.98|       54.5|           1|\n",
      "| 2016-03-01 00:00:00|        30.43|       98.0|           1|\n",
      "| 2016-03-01 00:00:03|        16.81|       52.0|           1|\n",
      "| 2016-03-01 00:00:07|        18.03|       52.0|           1|\n",
      "| 2016-03-01 00:00:21|        19.35|       52.0|           1|\n",
      "+--------------------+-------------+-----------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 3. Filtering Query (10 pts)\n",
    "# Requirement: Use WHERE with logical operators (AND/OR).\n",
    "# We find 'High Value' trips: Fare > $50 AND Distance > 10 miles.\n",
    "print(\"\\n--- Query 2: Filtering (High Value Trips) ---\")\n",
    "q2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        tpep_pickup_datetime,\n",
    "        trip_distance,\n",
    "        fare_amount,\n",
    "        payment_type\n",
    "    FROM taxi_trips\n",
    "    WHERE fare_amount > 50 AND trip_distance > 10\n",
    "\"\"\")\n",
    "q2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c2f775-2bd7-4f4a-b185-17a6247b0515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Join Query (10 pts)\n",
    "# Requirement: Join with a helper table.\n",
    "# We Create a small 'lookup table' for Payment Types (1=Credit Card, 2=Cash, etc.)\n",
    "# Then we JOIN it to our main data to get the text description.\n",
    "\n",
    "# A. Create the Lookup Data\n",
    "payment_data = [\n",
    "    (1, \"Credit Card\"),\n",
    "    (2, \"Cash\"),\n",
    "    (3, \"No Charge\"),\n",
    "    (4, \"Dispute\"),\n",
    "    (5, \"Unknown\"),\n",
    "    (6, \"Voided\")\n",
    "]\n",
    "payment_df = spark.createDataFrame(payment_data, [\"payment_type_id\", \"payment_desc\"])\n",
    "payment_df.createOrReplaceTempView(\"payment_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23565a94-be2f-446a-9d5d-fa33ab9e195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query 3: Join (Trips + Payment Description) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+------------+\n",
      "|VendorID|fare_amount|payment_type|payment_desc|\n",
      "+--------+-----------+------------+------------+\n",
      "|       1|        9.0|           1| Credit Card|\n",
      "|       1|       11.0|           1| Credit Card|\n",
      "|       2|       54.5|           1| Credit Card|\n",
      "|       2|       31.5|           1| Credit Card|\n",
      "|       2|       98.0|           1| Credit Card|\n",
      "|       2|       23.5|           1| Credit Card|\n",
      "|       1|        5.5|           1| Credit Card|\n",
      "|       2|       23.5|           1| Credit Card|\n",
      "|       1|        5.5|           1| Credit Card|\n",
      "|       2|        9.0|           1| Credit Card|\n",
      "+--------+-----------+------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# B. Perform the JOIN\n",
    "print(\"\\n--- Query 3: Join (Trips + Payment Description) ---\")\n",
    "q3 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        t.VendorID,\n",
    "        t.fare_amount,\n",
    "        t.payment_type,\n",
    "        p.payment_desc\n",
    "    FROM taxi_trips t\n",
    "    INNER JOIN payment_lookup p \n",
    "    ON t.payment_type = p.payment_type_id\n",
    "\"\"\")\n",
    "q3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74c08cff-f8cf-43ff-8a7d-5b022fbfaf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK 3: DATAFRAMES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Count: 12210952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Count:  12210952\n",
      "✅ Data cleaning complete (Rows with nulls removed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, count, max, desc\n",
    "\n",
    "print(\"=== TASK 3: DATAFRAMES ===\")\n",
    "\n",
    "# --- 1. Data Cleaning (10 pts) ---\n",
    "# Requirement: Handle missing values using .dropna() or .fillna().\n",
    "# We will drop rows where vital information (like passenger_count) is missing.\n",
    "cleaned_df = df.dropna(subset=[\"passenger_count\", \"trip_distance\", \"fare_amount\"])\n",
    "\n",
    "# Alternatively, we could fill nulls (uncomment to see):\n",
    "# cleaned_df = df.fillna(0, subset=[\"tolls_amount\"])\n",
    "\n",
    "print(f\"Original Count: {df.count()}\")\n",
    "print(f\"Cleaned Count:  {cleaned_df.count()}\")\n",
    "print(\"✅ Data cleaning complete (Rows with nulls removed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbc1908-071c-4044-b866-983251bae02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aggregation Results (Stats by Vendor) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+------------+\n",
      "|VendorID|Total_Trips|          Avg_Fare|Max_Distance|\n",
      "+--------+-----------+------------------+------------+\n",
      "|       1|    5731242|12.727348032067002|1.90726288E7|\n",
      "|       2|    6479710| 12.85498535891264|      390.07|\n",
      "+--------+-----------+------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --- 2. GroupBy + Aggregation (10 pts) ---\n",
    "# Requirement: Compute mean, max, count, etc. on a categorical field.\n",
    "# We group by 'VendorID' and calculate statistics for the Fare Amount.\n",
    "agg_df = cleaned_df.groupBy(\"VendorID\").agg(\n",
    "    count(\"*\").alias(\"Total_Trips\"),\n",
    "    avg(\"fare_amount\").alias(\"Avg_Fare\"),\n",
    "    max(\"trip_distance\").alias(\"Max_Distance\")\n",
    ")\n",
    "\n",
    "print(\"\\n--- Aggregation Results (Stats by Vendor) ---\")\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7ea5e2-65ea-4376-9aad-c171e0972a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sorting Results (High Fares by Passenger Count) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|passenger_count|fare_amount|trip_distance|\n",
      "+---------------+-----------+-------------+\n",
      "|              0|      252.3|          0.0|\n",
      "|              0|      230.0|          0.3|\n",
      "|              0|      210.0|          0.0|\n",
      "|              0|      190.0|          0.0|\n",
      "|              0|      150.0|          0.0|\n",
      "|              0|      148.0|          0.0|\n",
      "|              0|     144.71|          0.0|\n",
      "|              0|      141.0|        28.95|\n",
      "|              0|      141.0|          0.0|\n",
      "|              0|      140.5|         0.01|\n",
      "+---------------+-----------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --- 3. Sorting / OrderBy (10 pts) ---\n",
    "# Requirement: Sort by multiple columns.\n",
    "# We sort by 'passenger_count' (Ascending) and 'fare_amount' (Descending).\n",
    "# This shows us the most expensive trips for each passenger group size.\n",
    "sorted_df = cleaned_df.orderBy(col(\"passenger_count\").asc(), col(\"fare_amount\").desc())\n",
    "\n",
    "print(\"\\n--- Sorting Results (High Fares by Passenger Count) ---\")\n",
    "sorted_df.select(\"passenger_count\", \"fare_amount\", \"trip_distance\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a24fde-0a06-45fe-9a99-0bb2b4906752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sorting Results (0 Passengers Removed) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:================================================>       (13 + 2) / 15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|passenger_count|fare_amount|trip_distance|\n",
      "+---------------+-----------+-------------+\n",
      "|              1|  429496.72|          0.0|\n",
      "|              1|     2002.5|          0.0|\n",
      "|              1|     1120.5|          0.0|\n",
      "|              1|     1000.0|          0.0|\n",
      "|              1|      900.8|          0.0|\n",
      "|              1|      819.5|        160.8|\n",
      "|              1|      700.0|          0.0|\n",
      "|              1|      700.0|          0.4|\n",
      "|              1|     655.34|          0.8|\n",
      "|              1|      600.0|        138.1|\n",
      "|              1|      600.0|          0.2|\n",
      "|              1|      590.0|          0.0|\n",
      "|              1|      553.5|         2.75|\n",
      "|              1|      550.0|        196.0|\n",
      "|              1|      500.0|         19.2|\n",
      "|              1|      500.0|       110.74|\n",
      "|              1|      500.0|          2.8|\n",
      "|              1|      495.7|          0.0|\n",
      "|              1|      495.0|       126.54|\n",
      "|              1|      495.0|          0.0|\n",
      "+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Filter out 0 passengers first\n",
    "df_no_zero = cleaned_df.filter(cleaned_df[\"passenger_count\"] > 0)\n",
    "\n",
    "# Then sort\n",
    "sorted_df = df_no_zero.orderBy(col(\"passenger_count\").asc(), col(\"fare_amount\").desc())\n",
    "\n",
    "print(\"\\n--- Sorting Results (0 Passengers Removed) ---\")\n",
    "sorted_df.select(\"passenger_count\", \"fare_amount\", \"trip_distance\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25076e55-f364-40ad-b158-56855bb17557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
